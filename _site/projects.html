<h1>Projects</h1>

<ol>
  <li><a href="#connectivity">Functional connectivity research: can we find a common ground?</a></li>
  <li><a href="#trypophobia" title="Trypophobia is a fear and disgust reaction when seeing a clusters of small holes or bumps (maggots, infected skin). During this workshop we will create an artificial convolutional neural network that predicts if an image is likely to cause a trypophobic response. We will use Keras - a popular deep learning framework in Python.">Detecting trypophobia triggers [fully booked] </a></li>
  <li><a href="#game" title="Joint-action is a coordinated behavior of two or more individuals. There is an ongoing research concerning cognitive mechanisms behind it. This project is about developing a new tool for studying joint-action in the form of cooperative video game.">Development of video game for studying joint action dynamics </a></li>
  <li><a href="#movies">Training a human-like movie evaluation system based on the semantic features of the storylines [fully booked]</a></li>
  <li><a href="#whitematter">Hypothesis­-driven white matter tractography from T1­-weighted MRI images</a></li>
  <li><a href="#neuroon">One channel EEG sleep staging with open source and open hardware NeuroOn sleep mask</a></li>
  <li><a href="#ageingbiomarker">Building a brain-ageing biomarker using machine learning [fully booked] </a></li>
  <li><a href="#flypi" title="FlyPi, an open source, affordable, portable biology lab! Want to do behavioural tracking, optogenetics, diagnosis anywhere and on the cheap? Come join our workshop, we will build, learn how to use and develop interesting experiments for research and education.">Building and using the “FlyPi”: the 3D-printed Neurobiology Lab</a></li>
  <li><a href="#subcortex">Unfolding the Subcortex [new!]</a></li>
</ol>

<p><a id="connectivity"></a></p>

<hr />

<h2>Project 1: Functional connectivity research: can we find a common ground?</h2>

<h4>Natalia Bielczyk, Msc<sup>1</sup> <a href="mailto:natalia.bielczyk@gmail.com"><i class="fa fa-envelope"></i></a>  / Michał Bola, Phd <sup>2</sup></h4>

<ol>
  <li>Radboud University Nijmegen Medical Centre, Nijmegen, the Netharlands</li>
  <li>Nencki Institute of Experimental Biology, Warsaw, Poland</li>
</ol>

<p>Functional connectivity (FC) research has become one of the leading concepts used for characterising network dynamics across multiple disciplines, from neuroimaging, through gene expression networks, to social networks. It is also a basis for graph theoretical biomarkers of psychiatric disorders and as such, it become an important subfield of cognitive neuroimaging.</p>

<p>FC is usually operationalised by means of Pearson’s and partial correlation, however the implementation of FC can vary between different fields, and different applications. Then, there is a question: does an optimal method to quantify functional connectivity exist? Or is the choice dependent on the data properties? How to choose the right method? In this project, we will use open-access data from functional Magnetic Resonance Imaging, EEG, gene expression data, stock exchange data and a few other open-access datasets, and we will compare the leading methods for computing FC when applied to these datasets.</p>

<p>We will attempt to answer the questions: what are the pros and cons of different methods for quantifying FC? What are the differences and the similarities between different datasets, and how to choose the right method for the given dataset?</p>

<p><img src="/img/projects/project1.png?style=centerme" alt="test image size" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p><em>Fig.1. Different types of networks. A: a social network (Facebook); B: correlations on the stock exchange (106 companies listed at NASDAQ-100); C: a gene co-expression network (image adapted from http://wikipedia.org on CC BY-SA 3.0 license); D: large scale resting state networks in the brain (image adapted from Smith et al, 2009)</em></p>

<p><strong>A list of 1-5 key papers/materials summarising the subject</strong>:</p>
<ol>
  <li><a href="http://www.scholarpedia.org/article/Brain_connectivity">http://www.scholarpedia.org/article/Brain_connectivity</a></li>
  <li><a href="http://journal.frontiersin.org/article/10.3389/fnsys.2015.00175/full">Bastos, A. M., &amp; Schoffelen, J. M. (2015). A tutorial review of functional connectivity analysis methods and their interpretational pitfalls. Frontiers in systems neuroscience, 9.</a></li>
  <li>A. K. Enge, C. Gerloff,C. C. Hilgetag and G. Nolte (2013). Intrinsic Coupling Modes: Multiscale Interactions in Ongoing Brain Activity. Neuron 80 (4): 867–86</li>
  <li>M. Bola and V. Borchardt (2016). Cognitive Processing Involves Dynamic Reorganization of the Whole-Brain Network’s Functional Community Structure. Journal of Neuroscience 36 (13): 3633–5</li>
</ol>

<p><strong>A list of requirements for taking part in the project:</strong></p>
<ul>
  <li>BSc program, or higher</li>
  <li>English: good, not necessarily proficient</li>
  <li>programming languages / other competences: basics of Matlab, Python, LaTeX, basic statistics</li>
</ul>

<p><strong>A maximal number of participants</strong>: 10 (will be working in pairs)</p>

<p><strong>Skills and competences you can learn during the project</strong>:</p>

<ol>
  <li>looking for parallels in the datasets from different disciplines, representing the datasets with a model</li>
  <li>group project planning (we will discuss and divide tasks on the site)</li>
  <li>programming in a team, solving problems in parallel</li>
  <li>scientific writing (at least one paragraph per participant)</li>
</ol>

<p><strong>Is there a plan for extending this work to a paper in case the results are promising?</strong> Yes</p>

<p><a id="trypophobia"></a></p>

<hr />

<h2>Project 2: Detecting trypophobia triggers [fully booked]</h2>

<h4>Piotr Migdał, PhD<sup>1</sup> <a href="mailto:pmigdal@gmail.com"><i class="fa fa-envelope"></i></a></h4>

<ol>
  <li><a href="http://p.migdal.pl/">data science freelancer</a></li>
</ol>

<p>Trypophobia is a phobia of irregular patterns or clusters of small holes or bumps. It may arise from the sense of aversion towards skin infection with maggots or fungi. In general, this phenomenon is adaptive, because a strong sense of disgust may protect against touching infected humans, animals or corpses. Yet, it can also become maladaptive if some, otherwise benign, patterns cause a strong aversive response. During this workshop we will create an artificial convolutional neural network that predicts if an image is likely to cause a trypophobic response. Such networks are a state of the art technique for visual pattern detection.</p>

<p>The goal of the project is twofold:</p>

<ol>
  <li>provide a tool to filter or censor triggering images while browsing the Internet</li>
  <li>empirically explore which patterns contribute to this phenomenon, and potentially relate the results to analogous regions in the human visual cortex</li>
</ol>

<p>We will provide the data for this project. The initial results are promising, see <a href="https://github.com/grzegorz225/trypophobia-detector">this git repo</a>.</p>

<p><img src="/img/projects/project2.png?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" />
<em>Fig.2. The holes in lotus seed heads cause some anxiety in some people (source: Wikipedia)</em></p>

<p><strong>A list of 1-5 key papers/materials summarising the subject</strong>:</p>

<p>In this case, only the basic knowledge of trypophobia is required. An additional knowledge may help with giving a general context, but most likely won’t contribute to the solution during this event:</p>
<ol>
  <li><a href="https://en.wikipedia.org/wiki/Trypophobia">https://en.wikipedia.org/wiki/Trypophobia</a></li>
  <li><a href="https://www.reddit.com/r/trypophobia/">https://www.reddit.com/r/trypophobia/</a> (warning: triggers)</li>
</ol>

<p>Additionally, take a look at <a href="http://p.migdal.pl/2017/04/30/teaching-deep-learning.html">http://p.migdal.pl/2017/04/30/teaching-deep-learning.html</a>.
If you are new to Python, this <a href="http://www.southampton.ac.uk/~fangohr/teaching/python/book.html" title="Python for Computational Science and Engineering">book</a>  may be relevant.</p>

<p><strong>A list of requirements for taking part in the project</strong>:</p>
<ul>
  <li>BSc program, or higher</li>
  <li>English: good, not necessarily proficient</li>
  <li>programming languages / other competences: at least basics of Python (we will create a neural network in either Keras or PyTorch, modern frameworks for deep learning)</li>
</ul>

<p><strong>A maximal number of participants</strong>: 6 (1-2 per computer)</p>

<p><strong>Skills and competences you can learn during the project</strong>:</p>
<ol>
  <li>practical experience with deep learning for image classification</li>
  <li>insights into how artificial neural networks abstract visual information processing</li>
</ol>

<p><strong>Is there a plan for extending this work to a paper in case the results are promising?</strong> Yes</p>

<p><a id="game"></a></p>

<hr />

<h2>Project 3: Development of video game for studying joint action dynamics</h2>

<h4>Julian Zubek, PhD<sup>1</sup> <a href="mailto:zubekj@gmail.com"><i class="fa fa-envelope"></i></a> / Arkadiusz Białek, PhD<sup>1</sup></h4>
<ol>
  <li>Institute of Psychology, Jagiellonian University, Kraków</li>
</ol>

<p>The goal of the project is to create a playable video game for two players, which will be applied in a psychological experiment to measure capabilities for “joint action”—non accidental, coordinated behaviour of two or more individuals aimed at achieving their common goal.</p>

<p>A great many processes of different levels of complexity can be described in terms of joint action, from simple tasks such as carrying a heavy object together to playing a piano duet or engaging in linguistic exchange (Sebanz et al. 2006). Joint action tasks may be characterized by role distribution: there may be parallel roles (highly similar) or complementary roles (different, but interdependent) (Warneken et al. 2006). It is suggested that joint action processes are crucial to our development and survival as social species (Tomasello 2014). Identifying and understanding qualities of behavioural coordination and cognitive mechanisms governing joint action is an ongoing research endeavour. One promising approach is to construct an experimental task in the form of video game (Satta et al. 2017). This allows defining cooperation goals in the context of an artificial environment in which all aspects of environmental dynamics can be controlled. We can register specific actions performed by the players—such as cursor movements—and analyse them as interrelated time series in terms of synchronicity, recurrence, leader-follower relations etc. Such dynamical measures, when compared with other behavioural and psychological characteristics of the participants, may provide us deeper insights as to the factors determining quality of joint action.</p>

<p>Our game will be developed from scratch during the 2-day Brianhack, using ideas from participants. We will work on all aspects of game design: the concept, the graphics, programming, etc (while taking into account time constraints).</p>

<p><img src="/img/projects/project3.png" alt="image-title-here" class="img-responsive" height="100%" />
<em>Fig.3A. Screenshot from joint action game used in experiments by Satta et al 2007. Fig.3B. Parallel (a) and complementary (b) roles in joint action (Sebanz et al. 2006)</em></p>

<p><strong>The general requirements for the game are as follows:</strong></p>

<ul>
  <li>Game developed in Python+Kivy, multiple platform support</li>
  <li>Real time gameplay for two players</li>
  <li>Support for control using touchscreen</li>
  <li>Cooperative game in which both players try to achieve a common goal</li>
  <li>Game accessible to 6-7 year old children and their parents</li>
  <li>Possibility to measure different facets of joint action, i.e. containing parallel and complementary roles</li>
  <li>Optionally (only for parallel roles part of the game): single player mode (as control condition)</li>
</ul>

<p>While working on this project, we will gather insights into collaborative processes from two different perspectives: the perspective of a researcher planning an experiment, and the perspective of a group member engaged in collaborative task. Hopefully, this will be an enjoyable and stimulating experience.</p>

<p>General agenda:</p>
<ul>
  <li>Day 0: Getting to know each other.</li>
  <li>Day 1: Introduction and inspiration. Brainstorming session. Game outline. Introduction to game programming in Kivy. Useful design patterns.</li>
  <li>Day 2: Implementing the game. Preparing graphics. Testing. Wrap up.</li>
</ul>

<p><strong>A list of 1-5 key papers/materials summarising the subject:</strong></p>
<ol>
  <li>Satta, E., Ferrari-Toniolo, S., Visco-Comandini, F., Caminiti, R., &amp; Battaglia-Mayer, A. (2017). Development of motor coordination during joint action in mid-childhood. Neuropsychologia. https://doi.org/10.1016/j.neuropsychologia.2017.04.027</li>
  <li>Sebanz, N., Bekkering, H., &amp; Knoblich, G. (2006). Joint action: bodies and minds moving together. Trends in Cognitive Sciences, 10(2), 70–76. https://doi.org/10.1016/j.tics.2005.12.009</li>
  <li>Tomasello, M. (2014). The ultra-social animal. European Journal of Social Psychology, 44(3), 187–194. https://doi.org/10.1002/ejsp.2015</li>
  <li>Warneken, F., Chen, F., &amp; Tomasello, M. (2006). Cooperative activities in young children and chimpanzees. Child Development, 77(3), 640–663. https://doi.org/10.1111/j.1467-8624.2006.00895.x</li>
</ol>

<p><strong>A list of requirements for taking part in the project:</strong></p>
<ul>
  <li>BSc program, or higher</li>
  <li>Communicative English</li>
  <li>Some experience in Python programming and/or computer graphics and/or game design</li>
</ul>

<p><strong>A maximal number of participants:</strong> 6</p>

<p><strong>Skills and competences to be acquired during the project:</strong></p>
<ul>
  <li>Experimental design</li>
  <li>Creative design</li>
  <li>Basics of game programming in Python</li>
  <li>Good programming practices (test-driven development, pair programming, code review)</li>
</ul>

<p><strong>Is there a plan for extending this work to a paper in case the results are promising?</strong>
The developed game will be released as open source software. It will be used as one of the experimental tasks in an ongoing research project. Interested participants may be invited to further collaboration.</p>

<p><a id="movies"></a></p>

<hr />

<h2>Project 4: Training a human-like movie evaluation system based on the semantic features of the storylines [fully booked]</h2>

<h4>Julia Berezutskaya<sup>1,2</sup> <a href="mailto:ju.berezutskaya@gmail.com"><i class="fa fa-envelope"></i></a>  / Marcel van Gerven, PhD<sup>2</sup></h4>

<ol>
  <li>Brain Center Rudolf Magnus, University Medical Center Utrecht, The Netherlands</li>
  <li>Donders Institute for Brain, Cognition and Behaviour, Nijmegen, The Netherlands</li>
</ol>

<p>One of the interesting questions in behavioral neuroscience is how humans evaluate perceived complex information. For example, we would like to understand what makes a movie to be rated high or low on average. Specifically, it is interesting to see whether the information about the movie, such as the movie description, its cast, genre and other attributes, is predictive of the average rating the movie receives.</p>

<p>In our project, we want to develop a model that predicts IMDb ratings of the movies based on their descriptions. To our knowledge, the existing algorithms attempt to predict IMDb ratings based on meta-information about the movies: directors, actors, genre, year, movie length and so on (Hsu et al., 2014; San, 2016). We believe that one of the key indicators of the movie reception is the movie storyline along with its character descriptions. In the present project, we aim at building a model that predicts IMDb ratings using movie storyline information.</p>

<p>We propose to start with a baseline classifier, such as a random forest classifier, which takes in the key content words from the movie storyline description and predicts the IMDb rating of the movie. The performance of this model should tell us whether the simple ‘bag of words’ representation of the movie storyline can be predictive of the movie rating.</p>

<p>The second approach will entail training a discriminative deep neural network, which will take in the full storyline descriptions preserving the temporal relationships between the words, and predict the IMDb rating of the movie. The difference in model performance compared to the decision tree classifier should tell us whether the temporal dependences in the storyline description provide additional information about how the movie is going to be rated. It will also be interesting to see whether we will be able to retrieve more high-level semantic features from the text input, such as elements of the plot.</p>

<p>In both cases, we can make use of pretrained word embeddings to represent text information, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). In addition, IMDb movie tags, capturing various meta-information about the movies (e.g. genre, year of production and so on) can be included as additional predictors in both models.</p>

<p>We believe that in case of success this project can provide information about the trends in human behavior when it comes to evaluation of the movie input. It will be interesting to see whether we will be able to uncover the semantic features that influence the movie reception by public.</p>

<p><img src="/img/projects/project4.jpg?style=centerme" alt="image-title-here" class="img-responsive" height="100%" width="100%" align="center" /></p>

<p><em>Fig.4. ​Training​ ​a​ ​discriminative​ ​neural​ ​network​ ​to​ ​predict​ ​IMDb​ ​ratings.​ A.​ ​Schematic​ ​view​ ​of​ ​the​ ​artificial​ ​neural network​ ​(image​ ​from​ ​<a href="https://colah.github.io/posts/2015-01-Visualizing-Representations/">Christopher​ ​Olah,​ ​blog​ ​post</a>​).​ ​B.​ ​Word​ ​embeddings​ ​used​ ​as​ ​an​ ​input​ ​to​ ​the​ ​model​ ​(image from​ ​<a href="https://www.linkedin.com/pulse/brief-history-word-embeddings-some-clarifications-magnus-sahlgren">Magnus​ ​Sahlgren,​ ​linkedin​ ​post</a>​)</em></p>

<p><strong>A list of 1-5 key papers/materials summarising the subject:</strong></p>
<ol>
  <li>Hsu, P.-Y., Shen, Y.-H., and Xie, X.-A. (2014). Predicting Movies User Ratings with Imdb Attributes. In International Conference on Rough Sets and Knowledge Technology, (Springer), pp. 444–453.</li>
  <li>Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. ArXiv Prepr. ArXiv13013781.</li>
  <li>Pennington, J., Socher, R., and Manning, C.D. (2014). Glove: Global vectors for word representation. In EMNLP, pp. 1532–1543.</li>
  <li>San, C. (2016). Predict Movie Ratings: <a href="https://github.com/sundeepblue/movie_rating_prediction">https://github.com/sundeepblue/movie_rating_prediction</a></li>
</ol>

<p><strong>A list of requirements for taking part in the project:</strong></p>
<ul>
  <li>BSc program, or higher</li>
  <li>English: good, not necessarily proficient</li>
  <li>good Python programming skills and basic familiarity with machine learning and deep learning</li>
  <li>familiarity with Tensorflow, PyTorch or Chainer</li>
</ul>

<p><strong>A maximal number of participants:</strong> 5</p>

<p><strong>Is there a plan for extending this work to a paper in case the results are promising?</strong> yes</p>

<p><a id="whitematter"></a></p>

<hr />

<h2>Project 5: Hypothesis­-driven white matter tractography from T1­-weighted MRI images</h2>

<h4>Anastasia Osoianu<sup>1</sup> / Charl Linssen<sup>1</sup> <a href="mailto:charl@turingbirds.com"><i class="fa fa-envelope"></i></a> / Katja Heuer<sup>2</sup> / Roberto Toro, PhD<sup>3</sup></h4>

<ol>
  <li>Donders Institute for Brain, Cognition and Behaviour, Nijmegen, The Netherlands</li>
  <li>Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany</li>
  <li>Institut Pasteur, Paris, France</li>
</ol>

<p>Polarized light imaging (PLI) as well as the tractography of high angular resolution diffusion weighted imaging (DWI) data reveal a gross white matter (WM) geometry of striking regularity<a href="http://journal.frontiersin.org/researchtopic/168/wiring-principles-of-cerebral-cortex#articles" title="Frontier Research Topic &quot;Wiring Principles of Cerebral Cortex&quot;">[1]</a>,<a href="http://microdraw.pasteur.fr/microdraw.html?source=/vervet/vervet.json](http://microdraw.pasteur.fr/microdraw.html?source=/vervet/vervet.json" title="Vervet monkey brain slice scanned using Polarized Light Imaging">[3]</a>. This makes the neuroanatomist wonder whether it would be possible to generate a connectome based exclusively on a small set of hypotheses:</p>

<ul>
  <li>more than 90% of white matter connections are cortico­cortical</li>
  <li> the density of fibres is homogeneous throughout the white matter</li>
  <li>fibres are oriented perpendicular to gyral crowns and parallel to sulcal fundi</li>
  <li>fibres are sticky, which makes them aggregate in bundles of similar orientation.</li>
</ul>

<p>How much of a real brain connectome would be recovered by such a simple and reductionistic model?</p>

<p>We propose to approach this question in the simple case of a 2D coronal slice. We use methods inspired by swarm intelligence <a href="[http://www.red3d.com/cwr/boids/" title="Craig Reynolds' &quot;Boids&quot;">[2]</a>, and simulate a dynamical system where particles are instantiated at the gray matter/white matter boundary. The particles are allowed to propagate within the white matter mask, following simple rules that reflect the above list of hypotheses. After reaching a steady state, the white matter orientation distribution is reconstructed on a voxel-by-voxel basis, for each individual voxel based on the statistics of the particle paths crossing it. This is roughly the opposite of streamline tractography, where paths are sampled based on a distribution map. By implementing derived measures such as an anisotropy index, the resulting map can be quantitatively compared to empirical data. We are especially interested in multimodal effects that occur e.g. when two fibre bundles cross. We will validate our model on coronal slices of the vervet monkey, as an excellent empirical dataset is available <a href="http://microdraw.pasteur.fr/microdraw.html?source=/vervet/vervet.json" title="Vervet monkey brain slice scanned using Polarized Light Imaging">[3]</a>. Proof­ of­ concept code is available via GitHub<a href="https://github.com/aniv0s/FakeTensorImaging">[4]</a>.</p>

<p><img src="/img/projects/project5.jpg?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p><em>Fig.5. Visualization of a DTI measurement of a human brain. Depicted are reconstructed fiber tracts that run through the mid-sagittal plane (source: Thomas Schultz, Wikipedia).</em></p>

<p><strong>A list of 1-5 key papers/materials summarising the subject:</strong></p>

<ol>
  <li>Frontier Research Topic “Wiring Principles of Cerebral Cortex” — <a href="http://journal.frontiersin.org/researchtopic/168/wiring-principles-of-cerebral-cortex#articles">http://journal.frontiersin.org/researchtopic/168/wiring-principles-of-cerebral-cortex#articles</a></li>
  <li>Craig Reynolds’ “Boids” — <a href="">http://www.red3d.com/cwr/boids/</a>http://www.red3d.com/cwr/boids/]</li>
  <li>Vervet monkey brain slice scanned using Polarized Light Imaging — <a href="http://microdraw.pasteur.fr/microdraw.html?source=/vervet/vervet.json">http://microdraw.pasteur.fr/microdraw.html?source=/vervet/vervet.json</a></li>
  <li>Github repo <a href="https://github.com/aniv0s/FakeTensorImaging">https://github.com/aniv0s/FakeTensorImaging</a></li>
</ol>

<p><strong>A list of requirements for taking part in the project:</strong></p>

<ul>
  <li>familiarity with Python, Javascript or similar programming language</li>
  <li>affinity for simulating and analysing dynamical systems</li>
</ul>

<p><strong>A maximal number of participants on the project:</strong> 6</p>

<p><strong>Skills and competences you can learn during the project:</strong>
You will reflect on the organisational principles of white matter network connectivity across spatial scales, and formulate hypotheses about it. The simulation will be used to test as well as generate these hypotheses. The outcome of the simulation is continuously compared to that derived from PLI images.</p>

<p>Next to collective brainstorming, you can focus on any of two main themes in the project:</p>
<ol>
  <li>metrics and quantification, e.g. downloading and processing the PLI images; design the anatomical WM mask; computing derived measures such as anisotropy indices; comparison (e.g. Kullbeck-Leibler divergence) with the map generated by the simulation;</li>
  <li>development of the simulation method: what rules do particles propagate under? parameter optimisation using smart search (particle swarm optimisation on the parameters? meta-metaheuristic!), generate appropriate network graph (e.g. having small-world properties) that goes into the particle simulation as a boundary condition</li>
</ol>

<p><strong>Is there a plan for extending this work to a paper in case the results are promising?</strong> Yes</p>

<p><a id="neuroon"></a></p>

<hr />

<h2>Project 6: One channel EEG sleep staging with open source and open hardware NeuroOn sleep mask</h2>

<h4>Franciszek Rakowski PhD<sup>1</sup> <a href="mailto:rakowski@icm.edu.pl"><i class="fa fa-envelope"></i></a> / Paweł Kazimieczak<sup>2</sup></h4>
<ol>
  <li>ICM, University of Warsaw </li>
  <li><a href="https://inteliclinic.com/">Interclinic Co.</a></li>
</ol>

<p>This project means data science and your brain in daily practice!
Participants will have an opportunity to carry out EEG signal registration with the NeuroOn-Open mask. The NeuroOn-Open mask is a simple and portable device equipped with 2 EEG electrodes, pulse-oximeter and blood oxygen saturation measurement device.</p>

<p>Our project will consist of three steps:</p>
<ul>
  <li>performing the experiment (registration), with a kind help of firmware authors and a medical doctor</li>
  <li>a dedicated lecture on signal processing</li>
  <li>a coding time</li>
</ul>

<p>The coding will involve:</p>
<ol>
  <li>performing signal quality check</li>
  <li>designing and implementing features of the short epochs of the signal</li>
  <li>choosing the appropriate machine learning algorithm, and carrying out classification of the sleep epochs as belonging to the light, deep and REM sleep stages.</li>
</ol>

<p>The reference staging will be given by additional sleep stager (Philips Alice) or medical doctor examination.</p>

<p><img src="/img/projects/project6.png" alt="image-title-here" class="img-responsive" height="50%" /></p>

<p><em>Fig.6. Neuroon, sleep mask device (source: Neuroon).</em></p>

<p><strong>A list of 1-5 key papers summarising the subject:</strong></p>

<ol>
  <li>Neuroon Open hardware documentation <a href="https://github.com/inteliclinic/NeuroonOpenHardwareDocumentation/blob/master/Neurooon_Open_Hardware_Documentation_rev0002.pdf">https://github.com/inteliclinic/NeuroonOpenHardwareDocumentation/blob/master/Neurooon_Open_Hardware_Documentation_rev0002.pdf</a></li>
  <li>Benjamin D. Yetton, Mohammad Niknazar, Katherine A. Duggan, Elizabeth A. McDevitt, Lauren N. Whitehurst, Negin Sattari, Sara C. Mednick Automatic detection of rapid eye movements (REMs): A machine learning approach. Journal of Neuroscience Methods, 259 (2016)</li>
  <li>A Comparative Study on Classification of Sleep Stage Based on EEG Signals Using Feature Selection and Classification AlgorithmsBaha Şen &amp; Musa Peker &amp; Abdullah Çavuşoğlu &amp; Fatih V. Çelebi J Med Syst (2014)</li>
  <li>Neuron-Open on Kickstarter: <a href="https://www.kickstarter.com/projects/intelclinic/neuroon-open-smartest-sleep-dreams-and-meditation">https://www.kickstarter.com/projects/intelclinic/neuroon-open-smartest-sleep-dreams-and-meditation</a></li>
</ol>

<p><strong>A list of requirements for taking part in the project:</strong></p>
<ul>
  <li>BSc program, or higher</li>
  <li>English: good, not necessarily proficient</li>
  <li>programming languages / other competences: Python, SciKit-learn</li>
</ul>

<p><strong>A maximum number of participants:</strong> 20</p>

<p><strong>What can the participant gain from the project?</strong></p>
<ul>
  <li>solving problems to data acquisition on a firmware level</li>
  <li>signal processing techniques</li>
  <li>practical approach to machine learning methodology</li>
</ul>

<p><strong>Is there a plan for extending this work to a peer-reviewed paper in case the results are promising?</strong> No.
However, the results, or methods proposed at the Brainhack project might be implemented in commercially offered device, Neuroon-Med.</p>

<p><a id="ageingbiomarker"></a></p>

<hr />

<h2>Project 7: Building a brain-ageing biomarker using machine learning [fully booked]</h2>

<h4>James H Cole, PhD<sup>1</sup> <a href="mailto:james.cole@imperial.ac.uk"><i class="fa fa-envelope"></i></a> / Sebastian Popescu, MSc <sup>1</sup></h4>
<ol>
  <li>Computational, Cognitive &amp; Clinical Neuroimaging Laboratory (C3NL), Imperial College London</li>
</ol>

<p>As humans age, changes to the structure and function of the brain occur, so-called ‘brain ageing’. Brain ageing is associated with cognitive decline, decreased function capacity and a higher risk of neurodegenerative disease and dementia. A biomarker of the brain ageing process could have great utility in identifying people at risk of experiencing the adverse effects of brain ageing, before any symptoms manifest. Brain ageing biomarkers could also be useful for mapping individualised brain-ageing trajectories, assessing potential influences on brain ageing and in aiding the design of clinical trials.</p>

<p>Our work has used T1-MRI to design such a biomarker (brain-predicted age), taking voxelwise brain volume images and using a machine-learning regression to accurately predict chronological age in N=2001 healthy people aged 18-90. This follows the experimental design from biogerontology research that looks for measures of underlying ‘biological age’, and assesses the appropriateness of a measure based on the accuracy of age prediction. Our leading model has used voxelwise grey matter in a 3D convolutional neural network (CNN) approach, resulting in a mean absolute error (MAE) of 4.16 years, Pearson’s r = 0.96, R2 = 0.92. There is still considerable room for improvement in the model, to reduce the MAE towards minimal values. This is necessary if brain-predicted age is ever to have clinical impact, as currently the error levels mean that individualised predictions may be misleading.</p>

<p>The Hackathon project will encourage participants to develop their own brain-age prediction pipeline. Dataset #1 (N=2001) will be supplied, along with an independent validation set (dataset #2, N=650). These data will be in available in three formats: i) raw images, ii) FreeSurfer cortical thickness and subcortical volumes, iii) voxelwise grey matter and white matter volume images (derived from SPM). The participants can either use their own pre-processing pipeline or utilise the supplied processed datasets, then run any type of regression model of their choosing. This may or may not involve dimension reduction (e.g., PCA, clustering), feature selection (theory- or data-driven), use of kernels, regularisation and deep learning architectures.</p>

<p>The main dataset will be randomly partitioned into an 80-10-10% split, with separate samples for training, validation and testing. The final model will then also be assessed in dataset #2. The pipeline that results in the best test scores (i.e., MAE) for both dataset #1 and #2 will be declared the winner.
Finally, participants will be asked to consider ways of interpreting the feature importance, to help better understand the neuroanatomical features involved in the brain-age prediction.</p>

<p><img src="/img/projects/project7.jpg?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p><em>Fig.7. Similarity matrix of ~1700 images.</em></p>

<p><strong>A list of 1-5 key papers summarising the subject:</strong></p>
<ol>
  <li>Cole JH, Poudel RPK, Tsagkrasoulis D, et al. Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker. NeuroImage 2017. doi: 10.1016/j.neuroimage.2017.07.059</li>
  <li>Cole JH, Ritchie SJ, Bastin ME, et al. Brain age predicts mortality. Molecular psychiatry 2017. doi: 10.1038/mp.2017.62</li>
  <li>Franke K, Ziegler G, Klöppel S, Gaser C. Estimating the age of healthy subjects from T1-weighted MRI scans using kernel methods: Exploring the influence of various parameters. NeuroImage 2010; 50(3): 883-92.</li>
  <li>Konukoglu E, Glocker B, Zikic D, Criminisi A. Neighbourhood approximation using randomized forests. Medical Image Analysis 2013; 17(7): 790-804.</li>
  <li>Valizadeh SA, Hänggi J, Mérillat S, Jäncke L. Age prediction on the basis of brain anatomical measures. Human Brain Mapping 2017; 38(2): 997-1008.</li>
</ol>

<p><strong>A list of requirements for taking part in the project:</strong></p>
<ul>
  <li>conversational English</li>
  <li>Bachelor level of education</li>
  <li>basic understanding of statistical principles</li>
  <li>some experience of Machine Learning or other regression analysis in a language of their choice (e.g., python, Matlab, R or GUI-based software)</li>
</ul>

<p><strong>Maximum number of participants:</strong> 10</p>

<p><strong>What can the participant gain from the project?</strong>
Participants will gain an understanding of a key neuroscientific application of machine learning approaches, as well as an appreciation of the wider benefits of applying machine learning to biomedical problems. Particularly, participants will be encouraged to appreciate the importance of developing the whole analytic pipeline, rather than merely focusing on choice of machine learning algorithm. This includes considerations on pre-processing methods, feature selection and nested-cross-validation.</p>

<p><strong>Is there a plan for extending this work to a peer-reviewed paper in case the results are promising?</strong>
If a single prediction algorithm that generates a mean absolute error (MAE) less than the current leading application to these data (CNNs using voxelwise grey matter volume MAE = 4.16 years), then publication is warranted. The results of the different pipelines and algorithms used in the Hackathon will be then summarised and written up for submission to a peer-reviewed journal (e.g., NeuroImage, Human Brain Mapping, Frontiers in Aging Neuroscience), with all the Hackathon project participants included as co-authors, alongside the project team.</p>

<p><a id="flypi"></a></p>

<hr />

<h2>Project 8: Building and using the “FlyPi”: the 3D-printed Neurobiology Lab</h2>

<h4>Andre Maia Chagas<sup>1</sup> / Eric James McDermott<sup>2</sup> / Valerio Raco<sup>3</sup></h4>

<ol>
  <li>Centre for Integrative Neurosciences University of Tuebingen, Tuebingen, Germany</li>
  <li>International Max Planck Research School, Hertie Institute, University of Tuebingen, Tuebingen, Germany</li>
  <li>Universitätsklinikum Tuebingen, Tuebingen, Germany</li>
</ol>

<p>Small, genetically tractable species such as larval zebrafish, Drosophila, or Caenorhabditis elegans have become key model organisms in modern neuroscience. In addition to their low maintenance costs and easy sharing of strains across labs, one key appeal is the possibility to monitor single or groups of animals in a behavioural arena while controlling the activity of select neurons using optogenetic or thermogenetic tools. However, the purchase of a commercial solution for these types of experiments, including an appropriate camera system as well as a controlled behavioural arena, can be costly. Here, we present a low-cost and modular open-source alternative called the ‘FlyPi’.</p>

<p>Our design is based on a 3D-printed mainframe, a Raspberry Pi computer, and high-definition  camera  system as well as Arduino-based optical and thermal control circuits. Depending on the configuration, the FlyPi can be assembled for about €100 and features optional modules for light-emitting diode (LED)-based fluorescence microscopy and optogenetic stimulation as well as a Peltier-based temperature stimulator for thermogenetics. All functions of the FlyPi can be controlled through a custom-written graphical user interface. To demonstrate the FlyPi’s capabilities, we present its use in a series of state-of-the-art neurogenetics experiments. In addition, we demonstrate the FlyPi’s utility as a medical diagnostic tool as well as a teaching aid at Neurogenetics courses held at several African universities. Taken together, the low cost and modular nature as well as fully open design of the FlyPi make it a highly versatile tool in a range of applications, including the classroom, diagnostic centres, and research labs.</p>

<p><img src="/img/projects/project8.png?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p><em>Fig.8. Classroom teaching (A) and equipment improvisation. Optogenetic activation of Chrimson in Adult fruit fly (G).</em></p>

<p><strong>A list of 1-5 key papers / online materials summarising the subject:</strong></p>
<ol>
  <li><a href="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002086">Baden, T., Chagas, A. M., Gage, G., Marzullo, T., Prieto-Godino, L. L., &amp; Euler, T. (2015). Open Labware: 3-D printing your own lab equipment. PLoS biology, 13(3), e1002086.</a></li>
  <li><a href="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002086">Chagas, A. M., Prieto-Godino, L. L., Arrenberg, A. B., &amp; Baden, T. (2017). The€ 100 lab: A 3D-printable open-source platform for fluorescence microscopy, optogenetics, and accurate temperature control during behaviour of zebrafish, Drosophila, and Caenorhabditis elegans. PLoS biology, 15(7), e2002702.</a></li>
  <li><a href="https://hackaday.io/project/5059">Flypi - cheap microscope/experimental setup on hackday.io</a></li>
</ol>

<p><strong>A List of requirements for taking part in the project:</strong></p>
<ul>
  <li>Basic programming skills are advantageous but not mandatory</li>
  <li>basic English</li>
</ul>

<p><strong>Maximum number of participants:</strong>
20, working in pairs (participants do not need to apply in pairs, but this would be advantageous for building, maintaining and working with the FlyPi)</p>

<p><strong>What participants gain/learn from this  project:</strong>
In this project we want to teach participants how to build a DIY and affordable neurobiology lab. By having a hands on approach, participants will learn how to:</p>
<ul>
  <li>Solder electronic components</li>
  <li>Customise the user interface and program the device to do automated experiments</li>
  <li>Make affordable experiments using state-of-the-art neuroscience methods, such as optogenetics.</li>
  <li>Learn about open source hardware community and how to leverage OS technologies to make science more reliable, robust, and affordable.</li>
</ul>

<p><strong>Is there a plan for extending this work to a peer-reviewed paper in case the results are promising?</strong>
The project can be extended to a peer-reviewed paper when participants and project leaders think about educational literature.</p>

<p><a id="subcortex"></a></p>

<hr />

<h2>Project 9: Unfolding the Subcortex</h2>

<h4>Vinod Kumar, PhD<sup>1</sup></h4>
<ol>
  <li>Max Planck Institute for Biological Cybernetics, Tuebingen, Germany</li>
</ol>

<p>We all know how fascinating our brain is. It is even more fascinating once we try to understand how it works. Over the past few decades, neuroanatomists have been rigorously busy with charting this beautiful structure, ad trying to understand its complexity. With the advent of computers and graphics, the ability to illustrate the structure and function of the brain has achieved new heights.</p>

<p><img src="/img/projects/project9/image1.png?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p>For example, Van Essen and colleagues have achieved a fine level of cortical (Fig. 1) and cerebellar illustration (Fig. 2). Remarkably, they have demonstrated the unfolding of the complex cortical and cerebellar sheet into 2D (Fig. 1-2). This gives an incredible overview of our brain in just one shot. It also contributes to research as unfolding 3D pictures into 2D makes it easier to compare between brain activity across individuals.</p>

<p><img src="/img/projects/project9/image2.png?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p>However, there is still a room for improvement in this field, as to date, the subcortical structures have not been a subject to the unfolding procedures (Fig.3).</p>

<p><img src="/img/projects/project9/image3.png?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p>Therefore, in this project, we will work on the 2D unfolding of the subcortical structures in the brain. The goal of the project is to facilitate comprehension of the subcortical brain in one single view.</p>

<p><img src="/img/projects/project9/image4.png?style=centerme" alt="image-title-here" class="img-responsive" height="80%" width="80%" align="center" /></p>

<p><strong>A list of 1-5 key papers / online materials summarising the subject:</strong></p>
<ol>
  <li><a href="http://brainvis.wustl.edu/resources/VE_ANYAS02.pdf">Essen, D. C. (2002). Surface‐Based Atlases of Cerebellar Cortex in the Human, Macaque, and Mouse. Annals of the New York Academy of Sciences, 978(1), 468-479.</a></li>
  <li><a href="http://brainvis.wustl.edu/wiki/index.php/Caret:Operations/Morphing">http://brainvis.wustl.edu/wiki/index.php/Caret:Operations/Morphing</a></li>
  <li><a href="http://brainvis.wustl.edu/CaretHelpAccount/caret5_help/tutorials/Caret_Analysis_5.5.html">http://brainvis.wustl.edu/CaretHelpAccount/caret5_help/tutorials/Caret_Analysis_5.5.html</a></li>
</ol>

<p><strong>A List of requirements for taking part in the project:</strong></p>

<ul>
  <li>~~Graphics/Experience in working with 3D meshes i.e. cutting, flattening ~~</li>
  <li>~~ Blender or similar software knowledge.~~ (any programmer is good for the project.)</li>
  <li>English: good, not necessarily proficient</li>
  <li>Most importantly, an observant mind who knows when to google and learn on the go</li>
</ul>

<p><strong>Maximum number of participants:</strong>
5-6 (will be working in pairs)</p>

<p><strong>What participants gain/learn from this  project:</strong></p>

<ol>
  <li>Creativity design</li>
  <li>Communication skills</li>
  <li>Working in a team</li>
  <li>Neuroanatomy</li>
</ol>

<p><strong>Is there a plan for extending this work to a peer-reviewed paper in case the results are promising?</strong>
Yes</p>
